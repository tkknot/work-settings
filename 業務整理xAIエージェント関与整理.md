## 要件定義

**※仕様・要件は基本的にBacklogで管理・定義される**

- 要件を定義する必要があるのは以下パターン
  - **上位者主導（トップダウン）**
    - 仕様策定者から明確な仕様がアサインされるパターン
      - 主なタスク：内容の妥当性確認、抜け漏れチェック
  - **開発者主導（ボトムアップ）**
    - 関連課題からの派生・バグの発見・会議で確定した内容など、自分で要件を羅列するパターン
    - 技術的制約やコスト感による、代替仕様の提案（逆提案）
      - 主なタスク：実現可能な「複数のプラン」の策定
    - 非機能要件（システムの品質維持）の定義
      - 主なタスク：パフォーマンス、セキュリティ、リファクタリング計画の策定

- 手順 (Workflow)
  1. **Backlogの特定・確認**
     - AIにテーマに関連する既存課題を検索させる
     - 関連課題がある場合は内容を読み込み、議論の土台にする
  2. **要件の具体化と壁打ち**
     - 解決課題（Why）、ターゲット（Who）、価値（Value）を言語化する
     - 非機能要件（パフォーマンス・セキュリティ等）についても議論する
  3. **プラン策定と影響調査**
     - 実現プラン（松竹梅）とスコープ（やらないこと）を検討する
     - AIにコードベース検索で影響範囲を調査させる
     - 既存データとの整合性（マイグレーション要否など）を確認する
  4. **要件の妥当性と懸念点の総点検**
     - AIに「世間一般との乖離」「論理矛盾」「抜け漏れ」を指摘させる
     - 妥当性（課題解決になるか？）と懸念点（リスク）を最終チェックする
  5. **最終確認 (Human Check)**
     - 人間がGoサインを出す

- 進め方
  - 人間が集中すべきこと(人間のシングルタスクは前提)
    - **実行判断:** 提案された選択肢から最善策を決定し、Backlogの要件として確定させる
    - **価値の定義:** それを作って誰が喜ぶのか、本当に役に立つのかを考える
    - **責任の所在:** リリースして問題が起きた時に責任を取れるか判断する
    - **コンテキスト理解:** 書類には書かれていないチームの状況や歴史を考慮する
    - **直感的な違和感:** 「なんか変だな？」という勘や経験則に基づく違和感を大切にする
  - AIにやらせること（AIにはマルチタスクで同時並行させていい）
    - **Backlogの特定:** 議論のテーマに関連する既存の課題（過去の議論、重複しそうなバグ報告など）を検索して提示する
    - **Backlogの読解:** 指定されたBacklog課題をMCP経由で読み込み、内容を理解する
    - **論理的な矛盾の指摘:** 仕様書や既存コードとの矛盾点をロジカルに指摘する
    - **世間一般との乖離チェック:** その仕様やUXが、業界標準や一般的な感覚からズレていないか、「普通はどうするか」を照らし合わせて指摘する
    - **抜け漏れのチェック:** 課題内容に対し、「エラーの時は？」「スマホの場合は？」など細かいケースを洗い出し、追記を促す
    - **選択肢の整理:** 「理想的な方法」「現実的な方法」「とりあえず動く方法」などを比較できるようにまとめる
    - **影響の調査:** コードベースを検索し、ここを変えると他にどこがおかしくなるかを調べる
    - **全般的なレビュー:** 提案された要件全体を通して、分かりやすさ、目的との整合性、実現可能性などを総合的に評価する
    - **文章の清書:** 議論の結果を、Backlogにそのまま貼れるきれいな説明文として出力する

### commands

### agents

### rules

### skills


## 設計・実装

- 手順 (Workflow)
  1. **作業計画の策定**
     - 実装対象の特定、変更範囲の洗い出し
  2. **計画のリファクタリング（設計レビュー）**
     - 以下の観点でAIと壁打ちし、計画をブラッシュアップする
       - フレームワークの設計思想に沿っているか？
       - 最適な設計パターン（GoF, DDD, Clean Architecture等）の介入余地はあるか？
       - リファクタリングの余地はないか？
       - アーキテクチャと整合しているか？
       - 過剰な機能（YAGNI）はないか？
       - テスト容易性（TDD可能か？）の確認
  3. **TDDによる実装**
     - TDD可能な場合: テストケース作成 → Red-Green-Refactorサイクル
     - TDD困難な場合: プロトタイピング → 実装 → 後追いでテスト
  4. **実装後のリファクタリング (Final Refactoring)**
     - AI: 可読性・保守性の観点で最終的なコード整理を行う
  5. **人間による最終レビュー (Human Review)**
     - 完成したコード、テスト、ドキュメントが期待通りか、規約に沿っているかを確認し、マージ判断を下す

- 進め方
  - 人間が集中すべきこと
    - **アーキテクチャの意思決定:** AIが提示した設計案から、プロジェクトの文脈に最適なものを選択する
    - **コードレビューと承認:** AIが生成した複雑なロジックやテストコードが要件を満たしているか確認し、マージを承認する
    - **「手戻り」の判断:** 設計ミスに気付いた時、どこまで戻って修正するかを決める
  - AIにやらせること
    - **設計案の提示:** クリーンアーキテクチャ、レイヤードなど、要件に適した複数のアーキテクチャ案を提示する
    - **複雑なロジックの実装:** ドメイン知識が必要なコアロジックについても、仕様に基づき実装案を提示する
    - **設計パターンの提案:** フレームワークの規約やGoF/DDD等のパターンに基づき、最適な設計案を提示する
    - **過剰設計の指摘 (YAGNI):** 必要以上に複雑な構造や、使われない機能の可能性を指摘し、シンプルさを促す
    - **コード品質の計測:** 循環的複雑度やメソッド長などのメトリクスを測定し、リファクタリングの優先順位を示す
    - **テストコードの作成 (TDD):** 実装前にテストケースを提示し、実装に合わせてテストコードを書く
    - **リファクタリング提案:** 「関数が長すぎる」「重複がある」などのコードスメルを指摘し、修正案を出す
    - **エラー修正:** コンパイルエラーやテスト失敗の原因を特定し、修正コードを提案する


## 検証方法作成

- 手順 (Workflow)
  1. **テストシナリオの洗い出し (Scenario Identification)**
     - 要件定義（Backlog）に基づき、ユーザーが達成すべきゴール（ユースケース）を列挙する
     - AI: 要件から「ハッピーパス（正常系）」と「異常系」のシナリオリストを提案する
  2. **テスト設計技法の選定とケース設計 (Method Selection & Case Design)**
     - 対象の性質に応じた設計技法でテストケースを設計し、CSV形式で出力する
     - 設計技法の使い分け（AIが技法を提案し人間が承認する）
       - **同値分析:** 入力値を同じ振る舞いをするグループに分類し、代表値でテストする。入力フォームやバリデーションに適する
       - **境界値分析:** 同値クラスの境界（最小値・最大値・境界±1）に絞ってテストする。数値範囲や文字数制限に適する
       - **ディシジョンテーブル:** 複数の条件の組み合わせによって異なる結果になるロジックを表形式で整理する。権限・状態フラグなどの複合条件に適する
       - **ペアワイズ（組み合わせテスト）:** 全組み合わせを網羅する代わりに、2因子間の組み合わせを網羅する。多数のオプションやパラメータが絡む場合にテスト数を削減しつつ高い検出率を維持する
     - AI: 各技法に沿ったテストデータや期待値を補完し、CSV形式でケース記述を作成する
  3. **用語・表現のレビュー (Terminology Review)**
     - AIが生成したテストケースの表現が、QA担当者や非エンジニアに伝わる言葉になっているか確認する
     - AI: 自分が生成したテストケースを自己レビューし、コード上の変数名・システム内部識別子・技術用語が混入していないか確認して業務用語・画面上の表示名に置き換える
     - 人間: AIの自己レビュー後も残っている伝わりにくい表現を最終確認し、追加の修正を指示する
     - AI: 指摘を受けた箇所をさらに書き直す
  4. **E2E自動化の実装 (E2E Automation)**
     - 設計したテストケースのうち、自動化可能なものをPlaywright等でE2Eテストコードにする
     - 自動化が難しいケース（視覚的確認が必要なもの等）は手動テストとして残す
     - AI: テストコードのボイラープレート生成、セレクタの特定を行う
  5. **レビューと実行 (Review & Execution)**
     - 実装されたテストが正しく動作し、要件を網羅しているか確認する
     - 人間: テストの意図が合っているか、過剰/不足がないか最終確認する

- 進め方
  - 人間が集中すべきこと
    - **「何をテストすべきか」の決定:** ビジネス価値に直結する重要なシナリオを選定する（全部テストするのはコスト的に無理なため）
    - **設計技法の承認:** AIが提案した技法選定が、対象の性質に合っているか判断する
    - **用語・表現のチェック:** AIが生成したテストケースに、コード上の変数名やシステム識別子など、QAに伝わりにくい表現が混入していないか確認する
    - **自動化判断:** 設計したケースのうちどれをE2E自動化し、どれを手動テストで残すかを決める
    - **エッジケースの追加:** AIが見落としがちな、業務特有のレアケースや複雑な条件を追加する
    - **品質基準の決定:** 「どのブラウザで通ればOKか」「実行時間は何分以内か」などの合格ラインを決める
  - AIにやらせること
    - **設計技法の提案:** 対象の性質（入力値/条件分岐/組み合わせ等）を分析し、同値分析・境界値分析・ディシジョンテーブル・ペアワイズなど最適な技法を提案する
    - **シナリオの網羅的列挙:** 選定した技法に基づき、パターンを漏れなくリストアップする
    - **テストケースの自己レビューと表現改善:** ケース生成後に自らレビューし、コード上の識別子・技術用語を業務用語・画面表示名に置き換える。また人間からの追加指摘があれば再修正する
    - **E2Eテストコード生成:** 自動化対象のケースをPlaywright等のスクリプトとして生成する
    - **テストデータの準備:** テストに必要なユーザーデータやマスタデータの投入スクリプトを作成する


## バグ調査

- 手順 (Workflow)
  1. **現象の確認 (Issue Confirmation)**
     - Backlogの課題内容をMCP経由で取得し、報告内容、発生環境、期待値などを把握する
     - AI: チケット内容を要約し、調査に必要な情報が揃っているか確認する
  2. **現象の再現 (Reproduction)**
     - 報告されたバグを再現する最小限のコードやテストケースを作成する
     - AI: エラーログやユーザー報告から再現手順を推測し、再現コードを書く
  2. **原因の特定 (Root Cause Analysis)**
     - 再現コードを使ってデバッグし、問題の根本原因を特定する
     - AI: スタックトレースの解析、関連コードの検索、仮説の提示と検証を行う
  3. **修正方針の策定 (Fix Strategy)**
     - 対症療法ではなく、根本解決を目指す方針を決める
     - 人間: 修正による副作用のリスクを評価し、修正範囲を決定する
  4. **修正と検証 (Fix & Verify)**
     - TDDサイクル（Red-Green）で修正を行う
     - AI: 修正コードの提案、類似バグの検索（水平展開）

- 進め方
  - 人間が集中すべきこと
    - **ビジネスインパクトの判断:** そのバグの緊急度や、ユーザーへの影響範囲を評価する
    - **修正方針の決定:** 「とりあえず動くようにする」か「根本的に直す」かのトレードオフ判断
  - AIにやらせること
    - **Backlogの確認:** MCP経由で課題内容を取得し、現象・環境・期待値を要約する
    - **原因・影響範囲の特定:** ソースコードリーディングを通じて、バグの発生原因や修正による影響範囲を特定する
    - **再現コード生成:** バグを再現するテストケースを自動生成する
    - **修正案の提示:** 複数の修正パターン（Quick Fix vs Deep Fix）を提示する


